# Lab 02 Review Session - ORCHESTRATOR GUIDE

**Neural Networks Course - Computer Engineering**  
**Duration:** 90 minutes (1:30)  
**Format:** Interactive group work + discussions

---

## üìã SESSION OVERVIEW

**Purpose:** Solidify understanding of Labs 01-02 through interactive activities before moving to training

**Key Topics:**

1. Single neuron limitations (XOR problem)
2. Object-Oriented Programming fundamentals
3. MLP architecture design
4. Forward propagation mechanics
5. Real-world applications

**Materials Needed:**

- Printed student activity sheets (1 per student)
- Whiteboard markers (4-5 colors)
- Whiteboard space for each group
- Projector for displaying solutions
- Scratch paper for calculations

---

## ‚è±Ô∏è DETAILED TIMING BREAKDOWN

| Time      | Duration | Activity                          | Your Role         |
| --------- | -------- | --------------------------------- | ----------------- |
| 0:00-0:03 | 3 min    | Welcome & Overview                | Facilitator       |
| 0:03-0:18 | 15 min   | Part 1: Single Neuron Limitations | Monitor + Discuss |
| 0:18-0:43 | 25 min   | Part 2: OOP Concepts              | Support + Review  |
| 0:43-1:03 | 20 min   | Part 3: MLP Architecture          | Guide + Assess    |
| 1:03-1:23 | 20 min   | Part 4: Forward Propagation       | Verify + Explain  |
| 1:23-1:30 | 7 min    | Wrap-up & Assessment              | Summarize         |

**Total:** 90 minutes

---

## üé¨ MINUTE-BY-MINUTE ORCHESTRATION

### Opening (0:00 - 0:03) - 3 minutes

**What You Say:**

_"ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ! Welcome everyone. Today is different - no new material, no coding from scratch. Today is about YOU working together, discussing, and solidifying what we've learned."_

**Key Points to Hit:**

1. This is interactive - you'll work in groups
2. There are no "wrong" discussions - thinking process matters
3. I'll be walking around, listening, and helping
4. At the end, we'll share insights as a class

**Write on Board:**

```
TODAY'S AGENDA
‚îú‚îÄ Single Neuron: Why Not Enough?
‚îú‚îÄ OOP: Classes, Objects, Methods
‚îú‚îÄ MLP: Architecture Design
‚îú‚îÄ Math: Forward Propagation
‚îî‚îÄ Application: Real Problems
```

**Action:**

- Distribute student activity sheets
- Tell students to write their names on top
- "You have 90 minutes. Let's begin!"

---

## üìö PART 1: Single Neuron Limitations (0:03 - 0:18) - 15 minutes

### Individual Reflection (0:03 - 0:06) - 3 minutes

**What You Say:**

_"First, individual reflection. Take 3 minutes to answer Questions 1 and 2 on your sheet. This is from Lab 01 - testing your memory!"_

**Timer:** Set visible 3-minute timer

**Your Role:**

- Walk around silently
- Observe who's struggling vs confident
- Don't help yet - let them recall independently

**Expected Answers (for your reference):**

**Question 1:** _What does a single neuron do?_

```
GOOD ANSWER:
"A neuron calculates a weighted sum of inputs plus a bias,
then applies an activation function to produce an output."

ACCEPTABLE:
"Takes inputs, multiplies by weights, adds bias, applies function."

NEEDS HELP:
"Um... it processes data?" (too vague)
```

**Question 2:** _Formula?_

```
CORRECT:
output = f(Œ£(w·µ¢ √ó x·µ¢) + b)
OR
output = f(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b)

ACCEPTABLE:
"output = activation(weighted_sum + bias)"
```

### Group Discussion - XOR Challenge (0:06 - 0:18) - 12 minutes

**What You Say (0:06):**

_"Now, form groups of 3-4. You're about to see why single neurons break down. Read the XOR activity - you have 12 minutes total."_

**Group Formation (1 min):**

- Count off: "1, 2, 3, 4..." - all 1's together, all 2's together, etc.
- Or let students choose (faster but less diverse)

**Task Breakdown:**

**Minutes 0:07-0:11 (5 min):** Task 1 - Plot Points

**Your Actions:**

- Walk to each group
- Check if they're plotting correctly
- Quick visual check:

```
Correct Plot Should Look Like:

  B
  1 |  O (1,0)    ‚úì (1,1)
    |  [1]        [0]
    |
  0 |  ‚úì (0,0)   O (0,1)
    |  [0]        [1]
    |_____________
       0     1    A

‚úì = Output 0 (should be class 0)
O = Output 1 (should be class 1)
```

**If group is wrong:** Don't correct immediately. Ask: _"Which input is x-axis, which is y-axis? Check your XOR table."_

**Minutes 0:11-0:14 (3 min):** Task 2 - Draw Separating Line

**What You'll See:**

- Students trying different diagonal lines
- Frustration building (this is GOOD!)
- Some might say "impossible"

**Your Response:**

- _"Keep trying! Can you find ANY line?"_
- If they say impossible: _"Why do you think that?"_
- Let them struggle - the frustration is the learning moment

**Minutes 0:14-0:18 (4 min):** Task 3 - Group Discussion

**Listen for Key Insights:**

- "We can't separate them with one line"
- "We need a curved boundary"
- "Single neurons only make straight lines"
- "This is why we need multiple layers"

**Intervention Point (0:16):**

If groups are stuck, give a hint:

_"Think about Lab 01. A single neuron creates what kind of decision boundary? Can you draw that kind of boundary here?"_

### Class Debrief (0:18)

**What You Say:**

_"Time's up! Let's hear from groups. Who wants to share - could you draw a line?"_

**Expected Response:** "No! It's impossible!"

**Follow-up:** _"Why is it impossible?"_

**Wait for answers, then synthesize:**

_"Exactly! XOR is NOT LINEARLY SEPARABLE. A single neuron can only create straight decision boundaries. XOR needs a curved boundary. That's why we need..."_ (pause for students to say) _"...MULTIPLE LAYERS! Let's see how we build those with OOP."_

**Transition:** Move immediately to Part 2.

---

## üéÆ PART 2: OOP Concepts (0:18 - 0:43) - 25 minutes

### Individual Quick Check (0:18 - 0:23) - 5 minutes

**What You Say:**

_"Part 2 - Object-Oriented Programming. First, a quick matching exercise and fill-in-the-blanks. You have 5 minutes, work individually."_

**Set Timer:** 5 minutes

**Your Role:**

- Circulate and observe
- Note common mistakes for class discussion
- Don't give answers yet

**ANSWER KEY (for grading/discussion):**

**Matching:**

```
Class      ‚Üí C (Character creation screen/blueprint)
Object     ‚Üí A (Your actual character "ProGamer" with 87 health)
Attribute  ‚Üí D (Your character's health value)
Method     ‚Üí B (The action of moving)
self       ‚Üí E ("This specific player" - knowing YOUR stats)
```

**Fill in the blanks:**

```python
class Player:
    def __init__(self, name):        # 1st self
        self.name = name              # 2nd self
        self.health = 100             # 3rd self

    def shoot(self, target):          # 4th self
        self.ammo -= 1                # 5th self
        target.health -= 20
```

**Common Mistakes to Watch For:**

1. Using "this" instead of "self" (Java/C++ background)
2. Forgetting self parameter in method definition
3. Writing `name = name` instead of `self.name = name`

### Pair Programming Activity (0:23 - 0:43) - 20 minutes

**What You Say (0:23):**

_"Now pair up - find a partner. You're going to build a CoffeeDrink class together. One person types, one person guides. Switch every 5 minutes."_

**Pairing Strategy:**

- Strong student + struggling student (if you know students)
- OR: Random pairing
- OR: Let them choose

**Timeline Breakdown:**

**Minutes 0:23-0:33 (10 min):** Task 1 - Design the Class

**What You Do:**

- Project this starter code on screen (if available)
- Or write key parts on board

**Walk around and listen for:**

- Good discussions: "We need self.temperature = 'hot'"
- Confusion: "Where does self go?"
- Misunderstandings: "Do we need return in **init**?"

**Common Issues & Your Responses:**

**Issue 1:** "Do we write `temperature = 'hot'` or `self.temperature = 'hot'`?"

**Your Response:**
_"If you want each drink object to remember its own temperature, what should you use? Think about the Player class - did we write `health = 100` or `self.health = 100`?"_

**Issue 2:** "What should add_ice return?"

**Your Response:**
_"Does it need to return anything? Or does it just modify the object's state? Check the shoot method in Player class - did it return anything?"_

**Issue 3:** "How do we print in get_info?"

**Your Response:**
_"Use f-strings! Like: `print(f'{self.name} - {self.size} - {self.price} LE ({self.temperature})')`"_

**SOLUTION (for your reference - project at minute 0:33):**

```python
class CoffeeDrink:
    """Blueprint for coffee drinks at our caf√©"""

    def __init__(self, name, size, price):
        """Initialize a new coffee drink"""
        self.name = name
        self.size = size
        self.price = price
        self.temperature = "hot"  # Default temperature
        print(f"‚úì Created {size} {name}")

    def add_ice(self):
        """Make the drink iced"""
        self.temperature = "iced"
        print(f"{self.name} is now iced!")

    def get_info(self):
        """Display drink information"""
        print(f"{self.size} {self.name} - {self.price} LE ({self.temperature})")
```

**Minutes 0:33-0:38 (5 min):** Task 2 - Test Your Class

**What You Say (0:33):**

_"Okay, let's test! Create the two drinks and make the latte iced. Run your code."_

**Expected Output:**

```
‚úì Created Medium Latte
‚úì Created Large Cappuccino
Latte is now iced!
Medium Latte - 25 LE (iced)
Large Cappuccino - 30 LE (hot)
```

**If pairs are stuck:**

- "Did you create object instances? Like: `drink1 = CoffeeDrink(...)`?"
- "Are you calling methods on the objects? Like: `drink1.add_ice()`?"

**Project Solution:**

```python
# Test code
drink1 = CoffeeDrink("Latte", "Medium", 25)
drink2 = CoffeeDrink("Cappuccino", "Large", 30)

drink1.add_ice()
drink1.get_info()
drink2.get_info()
```

**Minutes 0:38-0:43 (5 min):** Task 3 - Reflection Discussion

**What You Say:**

_"Last 5 minutes - discuss the three reflection questions with your partner. I'll ask random pairs to share."_

**Walk around and listen. At 0:41, pick 2-3 pairs to share:**

**Question 1:** _"How is self used?"_

**Expected Answer:**
_"self refers to the specific object. It lets each drink remember its own name, price, temperature. Like self.temperature is THIS drink's temperature."_

**Question 2:** _"Difference between CoffeeDrink and drink1?"_

**Expected Answer:**
_"CoffeeDrink is the class (blueprint). drink1 is an object (actual instance). Class is written once, objects are created many times."_

**Question 3:** _"How to add add_sugar method?"_

**Expected Answer:**

```python
def add_sugar(self):
    print(f"Added sugar to {self.name}")
```

**Synthesis (0:43):**

_"Perfect! You now understand OOP basics. Remember: Class = Blueprint, Object = Instance, self = 'this object', Methods = Actions. Now let's use OOP to design neural networks!"_

---

## üß† PART 3: MLP Architecture Understanding (0:43 - 1:03) - 20 minutes

### Group Whiteboard Activity (0:43 - 0:58) - 15 minutes

**What You Say (0:43):**

_"Back to your original groups of 3-4. Each group gets whiteboard space. You're designing an MLP to predict if a student passes this course!"_

**Setup (2 min):**

- Assign whiteboard areas to groups
- Give each group markers
- "Read the problem description - you have 15 minutes total"

**Problem Recap (say aloud):**

_"You have 4 inputs: attendance, lab scores, midterm score, study hours. You output: pass or fail. Design the complete architecture!"_

**Timeline:**

**Minutes 0:45-0:52 (7 min):** Step 1 - Design Architecture

**Your Role:**

- Visit each group
- Ask guiding questions (don't give answers)

**What You'll See:**

**Group says: "4 inputs, 1 output, but how many hidden neurons?"**

**Your Response:**
_"Good question! There's no single right answer. What have you learned about hidden layer sizes? Think about the complexity of the problem."_

**Group says: "Should we use 1 or 2 hidden layers?"**

**Your Response:**
_"For this problem, 1 hidden layer is enough. But you could try 2! What matters is justifying your choice."_

**Common Architectures You'll See:**

```
Conservative:   [4, 4, 1]   - hidden = input size
Standard:       [4, 6, 1]   - hidden = 1.5x input
Ambitious:      [4, 8, 1]   - hidden = 2x input
Overcomplicated:[4, 8, 4, 1] - two hidden layers (ok but unnecessary)
```

**All are acceptable if justified!**

**CORRECT ANSWER (for your reference):**

```
Inputs: 4 (attendance, labs, midterm, hours)
Hidden: 6-8 neurons (good starting point)
Output: 1 (pass=1, fail=0)

Recommended: [4, 6, 1] or [4, 8, 1]
```

**Minutes 0:52-0:57 (5 min):** Step 2 - Draw Network

**Check each group's drawing looks something like:**

```
Correct Drawing Structure:

O   O   O   O     <-- Input (4 nodes)
 \ / \ / \ / \
  X   X   X   X   <-- Hidden (example: 4 nodes)
   \ / \ / \ /
      O           <-- Output (1 node)
```

**Common mistakes:**

- Not connecting all nodes between layers (remind: "fully connected!")
- Connecting within same layer (say: "neurons only connect to NEXT layer")

**Minutes 0:57-0:58 (3 min):** Step 3 - Calculate Parameters

**This is where math happens. Walk around with this formula:**

**Write on main board:**

```
PARAMETER CALCULATION

Weights in layer = (neurons_previous) √ó (neurons_current)
Biases in layer = neurons_current

Example: [4, 6, 1]

Layer 1 (Input‚ÜíHidden):
  Weights: 4 √ó 6 = 24
  Biases: 6
  Subtotal: 30

Layer 2 (Hidden‚ÜíOutput):
  Weights: 6 √ó 1 = 6
  Biases: 1
  Subtotal: 7

Total: 30 + 7 = 37 parameters
```

**Check groups' calculations:**

For [4, 6, 1]:

- Layer 1: 24 weights + 6 biases = 30
- Layer 2: 6 weights + 1 bias = 7
- **Total: 37 parameters** ‚úì

For [4, 8, 1]:

- Layer 1: 32 weights + 8 biases = 40
- Layer 2: 8 weights + 1 bias = 9
- **Total: 49 parameters** ‚úì

### Class Discussion (0:58 - 1:03) - 5 minutes

**What You Say (0:58):**

_"Markers down! Let's see what you designed. I want one person from each group to share: your architecture and WHY."_

**Call on 3-4 groups. For each:**

1. "What's your architecture?"
2. "Why did you choose that size hidden layer?"
3. "How many total parameters?"

**Listen for good justifications:**

- ‚úì "We chose 8 because it's 2x the input size, giving network enough capacity"
- ‚úì "We used 6 because it's between input and output, balanced"
- ‚úó "We just guessed" (push them: "But why that guess? What made you think that?")

**After sharing, synthesize:**

_"Great work! Notice: different groups chose different architectures, but all can work! What matters is:_

1. _Architecture matches problem (4 inputs, 1 output)_
2. _Hidden layer has reasonable size (not 1, not 100)_
3. _You can explain your choice_

_In real ML, we'd try multiple architectures and see which performs best. That's called hyperparameter tuning!"_

**Transition:**

_"You've designed the structure. Now let's see how data flows through it - forward propagation math!"_

---

## üî¢ PART 4: Forward Propagation Trace (1:03 - 1:23) - 20 minutes

### Individual Challenge (1:03 - 1:13) - 10 minutes

**What You Say (1:03):**

_"This is a math challenge. You have a tiny network [2, 2, 1] with specific weights and biases. Trace how input [1.0, 0.5] flows through to produce output. Work individually - show ALL your calculations!"_

**Set Timer:** 10 minutes

**Write on board (large and clear):**

```
NETWORK: [2, 2, 1]
INPUT: [1.0, 0.5]

LAYER 1 (Input ‚Üí Hidden):
Weights W¬π:  [[0.5, 0.3],      Neuron 1: [0.5, 0.3]
              [0.2, 0.4]]      Neuron 2: [0.2, 0.4]

Biases b¬π:   [-0.1, -0.2]

Activation: step(z) = 1 if z ‚â• 0, else 0

LAYER 2 (Hidden ‚Üí Output):
Weights W¬≤:  [[0.6],
              [0.8]]
Bias b¬≤:     [-0.3]

Activation: step(z)
```

**Your Role:**

- Circulate silently for first 5 minutes
- Let students struggle with calculations
- After 5 minutes, start giving hints if needed

**Common Issues & Hints:**

**Issue 1:** "I don't know where to start"

**Hint:**
_"Start with first hidden neuron. What's the formula? z = (w‚ÇÅ √ó x‚ÇÅ) + (w‚ÇÇ √ó x‚ÇÇ) + b"_

**Issue 2:** "I got z‚ÇÅ = 0.55, is that right?"

**Check their work:**

```
z‚ÇÅ = (0.5 √ó 1.0) + (0.3 √ó 0.5) + (-0.1)
   = 0.5 + 0.15 - 0.1
   = 0.55 ‚úó WRONG (should be 0.45)
```

**Say:** _"Recheck your arithmetic. 0.5 + 0.15 = ?"_

**Issue 3:** "What do I do with z after calculating it?"

**Hint:**
_"Apply the activation function! Step function: if z ‚â• 0, output 1, else output 0."_

**COMPLETE SOLUTION (for your reference):**

```
HIDDEN LAYER CALCULATIONS:

Neuron 1:
z‚ÇÅ = (0.5 √ó 1.0) + (0.3 √ó 0.5) + (-0.1)
   = 0.5 + 0.15 - 0.1
   = 0.45
a‚ÇÅ = step(0.45) = 1   ‚úì (because 0.45 ‚â• 0)

Neuron 2:
z‚ÇÇ = (0.2 √ó 1.0) + (0.4 √ó 0.5) + (-0.2)
   = 0.2 + 0.2 - 0.2
   = 0.2
a‚ÇÇ = step(0.2) = 1   ‚úì (because 0.2 ‚â• 0)

Hidden layer output: [1, 1]

OUTPUT LAYER CALCULATION:

z_out = (0.6 √ó 1) + (0.8 √ó 1) + (-0.3)
      = 0.6 + 0.8 - 0.3
      = 1.1
output = step(1.1) = 1   ‚úì (because 1.1 ‚â• 0)

FINAL PREDICTION: 1
```

### Pair Verification (1:13 - 1:23) - 10 minutes

**What You Say (1:13):**

_"Time to verify! Partner with the person next to you. Compare your answers. If you got different results, work together to find the correct answer."_

**Minutes 1:13-1:18 (5 min):** Pair Comparison

**Your Role:**

- Visit pairs that are debating
- Listen to their reasoning
- Guide them to find errors

**Example Intervention:**

**Student A:** "I got output = 0"
**Student B:** "I got output = 1"

**Your Response:**
_"Okay, let's trace back. What did you each get for the hidden layer output? ... Ah, Student A got [0, 1] and Student B got [1, 1]. So the error is in the hidden layer. Let's recalculate neuron 1 together..."_

**Minutes 1:18-1:21 (3 min):** Class Solution

**Project or write full solution on board** (use the solution above)

**Walk through step-by-step:**

_"Let's verify together. Hidden neuron 1:_

- _z‚ÇÅ = 0.5 times 1.0 = 0.5_
- _Plus 0.3 times 0.5 = 0.15_
- _Plus bias -0.1_
- _Total: 0.5 + 0.15 - 0.1 = 0.45_
- _Step function: 0.45 ‚â• 0? Yes! So a‚ÇÅ = 1_

_Hidden neuron 2:..._" (continue)

**Minutes 1:21-1:23 (2 min):** Reflection Challenge

**What You Say:**

_"Quick reflection: What if we changed input to [0, 1]? Talk to your partner - what would the output be? 30 seconds!"_

**After 30 sec, ask for answer:**

**SOLUTION for [0, 1]:**

```
z‚ÇÅ = (0.5 √ó 0) + (0.3 √ó 1) + (-0.1) = 0.2 ‚Üí a‚ÇÅ = 1
z‚ÇÇ = (0.2 √ó 0) + (0.4 √ó 1) + (-0.2) = 0.2 ‚Üí a‚ÇÇ = 1
z_out = (0.6 √ó 1) + (0.8 √ó 1) + (-0.3) = 1.1 ‚Üí output = 1
```

**Answer: Still 1!**

_"Interesting! Both [1.0, 0.5] and [0, 1] give output 1. This network isn't very discriminative with these weights. That's why we need TRAINING to find good weights!"_

---

## üéØ PART 5: Real-World Application (1:23 - 1:30) - 7 minutes

**What You Say (1:23):**

_"Final activity! Back to your groups. Choose one real-world problem and design an MLP for it. You have 5 minutes!"_

### Group Brainstorming (1:23 - 1:28) - 5 minutes

**Timer:** 5 minutes

**Your Role:**

- Quickly visit each group
- Make sure they chose a problem
- Push them to justify their architecture

**Expected Architectures:**

**Option A: Taxi Fare Predictor**

```
Inputs: 4 (distance, time, traffic, day)
Hidden: 6-8
Output: 1 (fare in LE)
Architecture: [4, 8, 1]

Challenge: Regression problem (continuous output), not classification
```

**Option B: Food Delivery Time**

```
Inputs: 5 (distance, items, prep time, traffic, weather)
Hidden: 8-10
Output: 1 (minutes)
Architecture: [5, 10, 1]

Challenge: Many factors interact, need decent hidden size
```

**Option C: Student Grade Predictor**

```
Inputs: 4 (hours, quizzes, attendance, assignments)
Hidden: 6-8
Output: 5 (A, B, C, D, F - one-hot encoding)
OR Output: 1 (single grade value)
Architecture: [4, 8, 5] or [4, 6, 1]

Challenge: Deciding classification vs regression
```

### Quick Presentations (1:28 - 1:30) - 2 minutes

**What You Say:**

_"Last 2 minutes! Two groups - 30 seconds each - share your problem and architecture!"_

**Call on 2 groups at random**

**Listen for:**

- Clear problem statement
- Reasonable architecture
- At least one identified challenge

**After presentations:**

_"Excellent! You're thinking like ML engineers. In the next lab, you'll learn how to TRAIN these networks to actually solve these problems. But you've now mastered the architecture design!"_

---

## ‚úÖ WRAP-UP & ASSESSMENT (1:30) - Closing

**What You Say:**

_"That's our 90 minutes! Before you leave, take 2 minutes to complete the self-assessment checklist on your sheet. Be honest with yourself."_

**While they check:**

**Write on board:**

```
KEY TAKEAWAYS TODAY:

‚úì Single neurons: Limited to linear boundaries
‚úì XOR: Classic non-linear problem
‚úì OOP: Classes (blueprints) vs Objects (instances)
‚úì self: Refers to specific object
‚úì MLP: Multiple layers = complex patterns
‚úì Forward prop: Layer-by-layer calculation
‚úì Architecture design: Match problem requirements
```

**Final Statement:**

_"You've now reviewed and APPLIED the foundational concepts. In Lab 03, we tackle the big question: How do we find the right weights? That's training - backpropagation and gradient descent. See you next time!"_

**Collect sheets (optional):**

If you want to assess participation, collect the activity sheets. Look for:

- ‚úì Completed calculations
- ‚úì Group discussion notes
- ‚úì Self-assessment honesty

---

## üìä POST-SESSION ASSESSMENT

### What to Look For (Indicators of Understanding):

**Strong Understanding:**

- ‚úì Students actively debating in groups
- ‚úì Correct XOR impossibility explanation
- ‚úì Using `self` correctly without prompting
- ‚úì Accurate forward propagation calculations
- ‚úì Justified architecture choices

**Needs More Support:**

- ‚úó Silent during group work
- ‚úó Still confusing class vs object
- ‚úó Arithmetic errors in forward prop
- ‚úó Random architecture choices without justification

### Common Misconceptions to Address in Next Lab:

1. **"More layers is always better"**

   - Clarify: More layers = more complexity, not always better

2. **"Hidden layer size doesn't matter"**

   - Emphasize: Too small = underfitting, too large = overfitting

3. **"Weights can be anything"**

   - Preview: Training finds optimal weights

4. **"Forward prop is all there is"**
   - Foreshadow: Backward prop (training) is coming!

---

## üéØ TROUBLESHOOTING GUIDE

### If Session is Running Behind:

**Option 1:** Cut Part 5 (Real-World Application)

- Skip group brainstorming
- Just mention real applications in wrap-up

**Option 2:** Reduce Part 2 (OOP) pair work

- Show solution after 5 minutes instead of 10
- Skip partner testing

**Option 3:** Speed up Part 4 (Forward Prop)

- Do one neuron together as class
- Let students complete rest for homework

### If Session is Running Ahead:

**Bonus Activity 1:** Code debugging exercise (on student sheet)

**Bonus Activity 2:** Architecture comparison

- Draw [4, 4, 1] vs [4, 16, 1] on board
- Discuss: Which has more capacity? More parameters? Better?

**Bonus Activity 3:** Activation function discussion

- "We used step function. What if we used sigmoid? ReLU?"

---

## üìã MATERIALS CHECKLIST

**Before Session:**

- [ ] Print student activity sheets (1 per student + 2 extras)
- [ ] Test markers on whiteboards
- [ ] Clear whiteboard space for all groups
- [ ] Set up projector (if using)
- [ ] Prepare solutions on slides or board templates

**During Session:**

- [ ] Timer visible to students
- [ ] Solutions ready to project at key moments
- [ ] Markers distributed to groups
- [ ] Walking space between groups

**After Session:**

- [ ] Collect sheets (if assessing)
- [ ] Note common misconceptions
- [ ] Prepare targeted review for next lab if needed

---

## üí° TEACHING TIPS

### Energy Management:

**High-Energy Moments:**

- Opening (get them excited)
- Group formations (make it quick and fun)
- Revealing XOR impossibility (build suspense)
- Class-wide solution reviews (interactive)

**Low-Energy Recovery:**

- If energy drops during calculations: "Stand up, stretch for 30 seconds!"
- Between parts: Quick 1-minute break

### Equity & Inclusion:

- **Ensure all voices heard:** Call on quiet students for sharing
- **Diverse groupings:** Mix strong and struggling students
- **Egyptian context:** Taxi, food delivery examples are relatable
- **Language:** Use Arabic terms when helpful (ŸÉŸÑÿßÿ≥ÿå ŸÉÿßÿ¶ŸÜÿå etc.)

### Handling Difficult Questions:

**"Why do we need OOP? Functions work fine!"**
‚Üí _"Try managing 100 players with functions. How many variables? How many parameters per function? OOP prevents that chaos."_

**"How do we know the right hidden layer size?"**
‚Üí _"Great question! Short answer: we experiment. Long answer: comes from experience and testing. No magic formula."_

**"This is too much math!"**
‚Üí _"I hear you. But look - you just traced a network by hand! That's exactly what computers do, just faster. Understanding the math means you control the AI, not the other way around."_

---

## üéì LEARNING OUTCOMES VERIFICATION

By end of session, students should demonstrate:

| Outcome                           | Verification Method         | Success Indicator                              |
| --------------------------------- | --------------------------- | ---------------------------------------------- |
| Explain single neuron limitations | XOR group discussion        | Can articulate linear separability             |
| Understand OOP concepts           | Coffee class implementation | Correctly uses self and attributes             |
| Design MLP architecture           | Student pass/fail network   | Matches inputs/outputs, reasonable hidden size |
| Trace forward propagation         | Individual calculation task | Correct arithmetic and logic flow              |
| Apply to real problems            | Group real-world brainstorm | Justifies architecture choices                 |

**If >70% of class demonstrates success indicators:** Well done! Ready for training lab.

**If <70% demonstrate success:** Plan targeted review session or adjust Lab 03 pacing.

---

## üìû SUPPORT RESOURCES

**For Students Struggling After Session:**

1. **Office hours:** Schedule 1-on-1 or small group
2. **Peer tutoring:** Pair struggling students with strong ones
3. **Online resources:**
   - 3Blue1Brown Neural Networks (YouTube)
   - Real Python OOP Tutorial
   - Interactive MLP visualizations

**For You (Instructor):**

1. **Review student sheets:** Identify patterns in misconceptions
2. **Adjust Lab 03:** Reinforce weak areas before introducing training
3. **Peer instructor consultation:** Discuss challenging moments

---

## ‚úÖ END-OF-SESSION SELF-CHECK (For You)

**Did I:**

- [ ] Keep to 90-minute timeframe (¬±5 min)?
- [ ] Ensure all groups participated actively?
- [ ] Address at least 3 different misconceptions?
- [ ] Connect each activity to neural networks clearly?
- [ ] Leave students confident and ready for training lab?

**What went well:**

```
_______________________________________________________________
```

**What to improve next time:**

```
_______________________________________________________________
```

**Students who need extra support:**

```
_______________________________________________________________
```

---

**You've got this! This session solidifies their foundation. Trust the structure, watch the time, and let students do the thinking!**

**Version:** 1.0  
**Course:** Neural Networks - Computer Engineering  
**Duration:** 90 minutes exactly

---

**END OF ORCHESTRATOR GUIDE**
