# Lab 02: Multi-Layer Perceptron - Instructor Guide (Part 2 of 3)

## Sections 3-5: Architecture, Mathematics & Implementation

**Prerequisites:** Complete Part 1 (Sections 1-2) first

---

## ğŸ—ï¸ SECTION 3: Multi-Layer Perceptron Architecture (45 minutes)

### 3.1 MLP Structure - The Building Blocks (15 minutes)

**Opening Statement:**
"Now that we understand OOP, let's learn how to build multi-layer neural networks. An MLP has three types of layers working together."

**Draw Large Diagram on Board:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MULTI-LAYER PERCEPTRON ARCHITECTURE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    INPUT LAYER      HIDDEN LAYER     OUTPUT LAYER
   (Not neurons,     (The magic        (Final
    just inputs)      happens here)     predictions)

      xâ‚ â”€â”€â”€â”€â”
             â”œâ”€â”€â”€â”€â”€â†’ hâ‚ â”€â”€â”€â”€â”€â”
      xâ‚‚ â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â†’ yâ‚
             â”œâ”€â”€â”€â”€â”€â†’ hâ‚‚ â”€â”€â”€â”€â”€â”¤
      xâ‚ƒ â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â†’ yâ‚‚
             â”œâ”€â”€â”€â”€â”€â†’ hâ‚ƒ â”€â”€â”€â”€â”€â”˜
             â”‚
             â””â”€â”€â”€â”€â”€â†’ hâ‚„

    3 inputs      4 neurons      2 outputs
```

**Explain Each Layer Type:**

**1. INPUT LAYER:**

**What to Say:**
"The input layer isn't really a 'layer' of neurons - it just holds your input data."

**Characteristics:**

- No weights, no biases, no activation
- Just data entry points
- Size = number of features in your data

**Examples (Write on board):**

```
Problem: Email Spam Detection
Inputs: 10 word frequencies â†’ Input layer size = 10

Problem: Image Recognition (28Ã—28 pixels)
Inputs: 784 pixel values â†’ Input layer size = 784

Problem: Weather Prediction
Inputs: [temp, humidity, pressure, wind] â†’ Input layer size = 4

Problem: XOR
Inputs: [A, B] â†’ Input layer size = 2
```

**2. HIDDEN LAYER(S):**

**What to Say:**
"Hidden layers are where the MAGIC happens! They extract patterns and features from the input."

**Characteristics:**

- Has weights and biases
- Applies activation functions
- Each neuron connected to ALL neurons in previous layer
- Can have multiple hidden layers (deep learning!)

**Why called 'hidden'?**
"We don't directly see what they do - they're 'hidden' between input and output. But they're learning useful representations of the data!"

**Example Roles:**

```
Image Recognition Hidden Layers:
Layer 1: Detects edges and basic shapes
Layer 2: Detects features (eyes, nose, mouth)
Layer 3: Detects faces
Output: Identifies person

Text Analysis Hidden Layers:
Layer 1: Detects letter patterns
Layer 2: Detects word patterns
Layer 3: Detects sentence meaning
Output: Sentiment (positive/negative)
```

**How many neurons?**
"There's no perfect formula! Common approaches:

- Start with input_size Ã— 2 or input_size / 2
- Experiment and see what works
- More neurons = more capacity (but slower, might overfit)"

**3. OUTPUT LAYER:**

**What to Say:**
"The output layer produces your final predictions. Its size depends on your problem type."

**Design Rules:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         OUTPUT LAYER SIZE GUIDE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Problem Type              â”‚ Output Size â”‚ Example   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Binary Classification     â”‚      1      â”‚ Spam: Y/N â”‚
â”‚  (two classes)            â”‚   or 2      â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Multi-Class Classificationâ”‚   # classes â”‚ Digit 0-9:â”‚
â”‚  (3+ classes)             â”‚             â”‚ 10 outputsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Regression                â”‚      1      â”‚ House $   â”‚
â”‚  (predicting a number)    â”‚             â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Multi-Output              â”‚  # outputs  â”‚ RGB color:â”‚
â”‚  (multiple predictions)   â”‚             â”‚ 3 outputs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Complete Examples:**

```
Example 1: XOR Problem
â”œâ”€ Inputs: 2 (A and B)
â”œâ”€ Hidden: 2 neurons (minimum to solve XOR)
â”œâ”€ Output: 1 (result: 0 or 1)
â””â”€ Architecture: [2, 2, 1]

Example 2: Iris Flower Classification
â”œâ”€ Inputs: 4 (sepal length/width, petal length/width)
â”œâ”€ Hidden: 8 neurons (good starting point)
â”œâ”€ Output: 3 (Setosa, Versicolor, Virginica)
â””â”€ Architecture: [4, 8, 3]

Example 3: Handwritten Digit Recognition (MNIST)
â”œâ”€ Inputs: 784 (28Ã—28 pixel image)
â”œâ”€ Hidden Layer 1: 128 neurons
â”œâ”€ Hidden Layer 2: 64 neurons
â”œâ”€ Output: 10 (digits 0-9)
â””â”€ Architecture: [784, 128, 64, 10]
```

![MLP architecture diagram showing input (3 nodes), hidden (4 nodes), output (2 nodes) with all connections](../images/mlp-architecture-diagram.png)

---

### 3.2 Network Notation and Representation (10 minutes)

**Standard Notation:**

**What to Say:**
"We represent network architecture as a simple list of numbers."

**Format:**

```
[input_size, hidden1_size, hidden2_size, ..., output_size]
```

**Examples on Board:**

```
[2, 2, 1]
 â†“  â†“  â†“
 â”‚  â”‚  â””â”€ 1 output neuron
 â”‚  â””â”€â”€â”€â”€ 2 neurons in hidden layer
 â””â”€â”€â”€â”€â”€â”€â”€ 2 input features

[4, 8, 3]
 â†“  â†“  â†“
 â”‚  â”‚  â””â”€ 3 output neurons (3 classes)
 â”‚  â””â”€â”€â”€â”€ 8 neurons in hidden layer
 â””â”€â”€â”€â”€â”€â”€â”€ 4 input features

[784, 128, 64, 10]
  â†“    â†“   â†“   â†“
  â”‚    â”‚   â”‚   â””â”€ 10 outputs (digits 0-9)
  â”‚    â”‚   â””â”€â”€â”€â”€â”€ 64 neurons in hidden layer 2
  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ 128 neurons in hidden layer 1
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 784 inputs (28Ã—28 pixels)

[10, 8, 4, 1]
 â†“   â†“  â†“  â†“
 â”‚   â”‚  â”‚  â””â”€ 1 output (binary: spam/not spam)
 â”‚   â”‚  â””â”€â”€â”€â”€ 4 neurons in hidden layer 2
 â”‚   â””â”€â”€â”€â”€â”€â”€â”€ 8 neurons in hidden layer 1
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10 inputs (word frequencies)
```

**Real Example - Email Spam Classifier:**

Draw detailed architecture:

```
EMAIL SPAM CLASSIFIER
Architecture: [10, 8, 4, 1]

INPUT LAYER (10 features):
â”œâ”€ Feature 1: Frequency of "free"
â”œâ”€ Feature 2: Frequency of "money"
â”œâ”€ Feature 3: Frequency of "click"
â”œâ”€ Feature 4: Number of exclamation marks!!!
â”œâ”€ Feature 5: Number of ALL CAPS words
â”œâ”€ Feature 6: Email length
â”œâ”€ Feature 7: Number of links
â”œâ”€ Feature 8: Sender reputation score
â”œâ”€ Feature 9: Time of day sent
â””â”€ Feature 10: Has attachments (0/1)

HIDDEN LAYER 1 (8 neurons):
â”œâ”€ Each neuron looks for patterns in all 10 features
â”œâ”€ Might learn: "lots of 'free' + 'money' + links = suspicious"
â”œâ”€ Might learn: "short email + known sender = probably safe"
â””â”€ Activation: ReLU (fast and effective)

HIDDEN LAYER 2 (4 neurons):
â”œâ”€ Combines patterns from Layer 1
â”œâ”€ Higher-level pattern detection
â””â”€ Activation: ReLU

OUTPUT LAYER (1 neuron):
â”œâ”€ Final decision: Spam or Not Spam
â”œâ”€ Output: probability between 0 and 1
â”œâ”€ Activation: Sigmoid
â””â”€ Decision rule: If > 0.5 â†’ Spam, else â†’ Not Spam
```

![Phish Responder: A Hybrid Machine Learning Approach to Detect Phishing and Spam Emails](../images/asi-05-00073-g002.png)

**Key Concept - Fully Connected:**

**What to Say:**
"Notice every neuron connects to ALL neurons in the next layer. This is called FULLY CONNECTED or DENSE layers."

Draw small example:

```
Layer 1        Layer 2
(2 neurons)    (3 neurons)

   nâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ mâ‚
      \       â•±
       \     â•±
        \   â•±
         \ â•±
          â•³
         â•± \
        â•±   \
       â•±     \
      â•±       â•²
   nâ‚‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ mâ‚‚
                â†“
                mâ‚ƒ

Each of the 2 neurons connects to ALL 3 neurons
Total connections: 2 Ã— 3 = 6 connections (weights)
```

---

### 3.3 Counting Parameters - Important Concept! (20 minutes)

**Why This Matters:**

**What to Say:**
"Understanding parameter count is CRUCIAL because:

1. More parameters = more memory needed
2. More parameters = slower training
3. Too many parameters = might overfit
4. Modern networks have MILLIONS of parameters!"

**The Formula:**

Write prominently on board:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARAMETERS PER LAYER                             â”‚
â”‚                                                   â”‚
â”‚  Weights:  num_inputs Ã— num_neurons               â”‚
â”‚  Biases:   num_neurons                            â”‚
â”‚                                                   â”‚
â”‚  Total:    (num_inputs Ã— num_neurons) + num_neuronsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example 1: Simple [2, 2, 1] Network (XOR)**

Work through this step-by-step on board:

```
Architecture: [2, 2, 1]

LAYER 1: Input (2) â†’ Hidden (2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weights: 2 inputs Ã— 2 neurons = 4 weights
  wâ‚â‚, wâ‚â‚‚  (weights for neuron 1)
  wâ‚‚â‚, wâ‚‚â‚‚  (weights for neuron 2)

Biases: 2 neurons = 2 biases
  bâ‚  (bias for neuron 1)
  bâ‚‚  (bias for neuron 2)

Layer 1 Total: 4 + 2 = 6 parameters

LAYER 2: Hidden (2) â†’ Output (1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weights: 2 inputs Ã— 1 neuron = 2 weights
  wâ‚, wâ‚‚  (weights for output neuron)

Biases: 1 neuron = 1 bias
  b  (bias for output neuron)

Layer 2 Total: 2 + 1 = 3 parameters

NETWORK TOTAL: 6 + 3 = 9 parameters
```

**Visualize the Parameters:**

Draw detailed diagram:

```
        INPUT          HIDDEN         OUTPUT
         xâ‚              hâ‚             y
                    wâ‚â‚â†—   â†˜wâ‚
         xâ‚‚         â•±       â•²
                wâ‚â‚‚â•±    bâ‚   â•²
                  â•±           â•²
                 â•±             â†˜
               hâ‚‚
           wâ‚‚â‚â†—   â†˜wâ‚‚         b
              â•±     â•²
         wâ‚‚â‚‚â•±   bâ‚‚  â•²

Parameters:
wâ‚â‚, wâ‚â‚‚, bâ‚  (3 params for hâ‚)
wâ‚‚â‚, wâ‚‚â‚‚, bâ‚‚  (3 params for hâ‚‚)
wâ‚, wâ‚‚, b     (3 params for y)
Total: 9 parameters
```

**Example 2: Practical [4, 8, 3] Network (Iris)**

```
Architecture: [4, 8, 3]

LAYER 1: Input (4) â†’ Hidden (8)
Weights: 4 Ã— 8 = 32
Biases:  8
Total:   40 parameters

LAYER 2: Hidden (8) â†’ Output (3)
Weights: 8 Ã— 3 = 24
Biases:  3
Total:   27 parameters

NETWORK TOTAL: 40 + 27 = 67 parameters
```

**Example 3: Deep Network [784, 128, 64, 10] (MNIST)**

```
Architecture: [784, 128, 64, 10]

LAYER 1: 784 â†’ 128
Weights: 784 Ã— 128 = 100,352
Biases:  128
Total:   100,480 parameters

LAYER 2: 128 â†’ 64
Weights: 128 Ã— 64 = 8,192
Biases:  64
Total:   8,256 parameters

LAYER 3: 64 â†’ 10
Weights: 64 Ã— 10 = 640
Biases:  10
Total:   650 parameters

NETWORK TOTAL: 100,480 + 8,256 + 650 = 109,386 parameters!
```

**Shocking Fact:**

**Say with emphasis:**
"This MNIST network has over 100,000 parameters! And this is considered a SMALL network. Modern image recognition networks like ResNet have MILLIONS of parameters!"

**Interactive Exercise:**

Give students this architecture and have them calculate:

```
Your Turn: Calculate parameters for [10, 20, 15, 5]

Layer 1: 10 â†’ 20
  Weights: _____ Ã— _____ = _____
  Biases: _____
  Total: _____

Layer 2: 20 â†’ 15
  Weights: _____ Ã— _____ = _____
  Biases: _____
  Total: _____

Layer 3: 15 â†’ 5
  Weights: _____ Ã— _____ = _____
  Biases: _____
  Total: _____

Network Total: _____
```

**Solution (reveal after 2 minutes):**

```
Layer 1: 10 Ã— 20 + 20 = 200 + 20 = 220
Layer 2: 20 Ã— 15 + 15 = 300 + 15 = 315
Layer 3: 15 Ã— 5 + 5 = 75 + 5 = 80

Total: 220 + 315 + 80 = 615 parameters
```

**Design Implications:**

Write on board:

```
ARCHITECTURE TRADE-OFFS:

Wide Networks (many neurons per layer):
âœ“ Can learn complex patterns quickly
âœ— Many parameters (memory intensive)
âœ— Slower to compute
Example: [10, 100, 5]

Deep Networks (many layers):
âœ“ Can learn hierarchical features
âœ“ More expressive with fewer parameters
âœ— Harder to train
Example: [10, 20, 20, 20, 5]

Balanced:
âœ“ Good starting point
âœ“ Reasonable parameter count
Example: [10, 30, 20, 5]
```

![Weight matrix visualization showing connections as a matrix](../images/weight-matrix-visualization.png)

---

## ğŸ“ SECTION 4: Mathematics of Forward Propagation (45 minutes)

### 4.1 Layer-by-Layer Computation - The Core Algorithm (20 minutes)

**Opening Statement:**
"Now we understand the structure. Let's learn the mathematics - how data flows through the network. This is called FORWARD PROPAGATION."

**The General Formula:**

Write prominently on board in a box:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     FORWARD PROPAGATION FOR ONE LAYER              â”‚
â”‚                                                    â”‚
â”‚  For layer â„“:                                      â”‚
â”‚                                                    â”‚
â”‚  z^(â„“) = W^(â„“) Ã— a^(â„“-1) + b^(â„“)   [Net input]    â”‚
â”‚                                                    â”‚
â”‚  a^(â„“) = f(z^(â„“))                  [Activation]   â”‚
â”‚                                                    â”‚
â”‚  Where:                                            â”‚
â”‚  - â„“ is the layer number                           â”‚
â”‚  - W^(â„“) are the weights for layer â„“              â”‚
â”‚  - a^(â„“-1) is activation from previous layer      â”‚
â”‚  - b^(â„“) are the biases for layer â„“               â”‚
â”‚  - f is the activation function                    â”‚
â”‚  - a^(â„“) is the output of layer â„“                 â”‚
â”‚                                                    â”‚
â”‚  Note: a^(0) = input features                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Symbol Explanation - Go Slowly:**

```
SYMBOL MEANINGS:

â„“ (script l)
  â””â”€ Layer number (superscript in parentheses)
  â””â”€ Example: Layer 1, Layer 2, Layer 3
  â””â”€ Helps us track which layer we're in

z^(â„“)
  â””â”€ Net input vector for layer â„“
  â””â”€ Result BEFORE activation function
  â””â”€ "Raw" neuron outputs
  â””â”€ Also called "pre-activation"

W^(â„“)
  â””â”€ Weight MATRIX for layer â„“
  â””â”€ Contains ALL weights for that layer
  â””â”€ Shape: (num_neurons) Ã— (num_inputs)

a^(â„“-1)
  â””â”€ Activation from PREVIOUS layer (â„“-1)
  â””â”€ This becomes INPUT to current layer
  â””â”€ For first layer: a^(0) = original inputs

b^(â„“)
  â””â”€ Bias VECTOR for layer â„“
  â””â”€ One bias per neuron in layer
  â””â”€ Shifts the activation

f
  â””â”€ Activation function (sigmoid, ReLU, tanh, etc.)
  â””â”€ Applied element-wise to z^(â„“)

a^(â„“)
  â””â”€ Activation output of layer â„“
  â””â”€ Result AFTER activation function
  â””â”€ Becomes input to next layer

Bold letters (W, a, b, z)
  â””â”€ Indicates vectors or matrices
  â””â”€ Multiple values, not just one number
```

**Step-by-Step Breakdown:**

Draw flowchart on board:

```
FORWARD PROPAGATION FLOW:

Previous Layer Output
        â”‚
        â†“
   a^(â„“-1)  â†â”€â”€ (this is our input)
        â”‚
        â†“
   [Multiply by weights]
        â”‚
   W^(â„“) Ã— a^(â„“-1)
        â”‚
        â†“
   [Add biases]
        â”‚
   W^(â„“) Ã— a^(â„“-1) + b^(â„“)
        â”‚
        â†“
      z^(â„“)  â†â”€â”€ (net input)
        â”‚
        â†“
   [Apply activation]
        â”‚
      f(z^(â„“))
        â”‚
        â†“
      a^(â„“)  â†â”€â”€ (output to next layer)
        â”‚
        â†“
   Next Layer Input
```

---

### 4.2 Concrete Example - XOR Solution (25 minutes)

**Important:** Work through this COMPLETELY on the board with actual numbers!

**Setup:**

**What to Say:**
"Let's solve XOR using a [2, 2, 1] network. I'll show you EXACTLY how the computation works with real numbers."

**Network Architecture:**

Draw on board:

```
XOR NETWORK: [2, 2, 1]

Input Layer (2)  Hidden Layer (2)  Output Layer (1)
      xâ‚             hâ‚                 y
      xâ‚‚             hâ‚‚
```

**Given Weights (These solve XOR!):**

```
LAYER 1 WEIGHTS (Input â†’ Hidden):

W^(1) = [[1.0, 1.0],    â† weights for hâ‚
         [1.0, 1.0]]    â† weights for hâ‚‚

b^(1) = [-0.5, -1.5]    â† biases for [hâ‚, hâ‚‚]

LAYER 2 WEIGHTS (Hidden â†’ Output):

W^(2) = [[1.0, -2.0]]   â† weights for y

b^(2) = [-0.5]          â† bias for y

Activation: sigmoid(z) = 1/(1 + e^(-z))
```

**Test Input: [1, 0] (XOR should output 1)**

**LAYER 1 COMPUTATION - Hidden Layer:**

```
Step 1: Calculate z^(1) (net inputs for hidden neurons)

For hâ‚:
  zâ‚^(1) = (wâ‚â‚ Ã— xâ‚) + (wâ‚â‚‚ Ã— xâ‚‚) + bâ‚
         = (1.0 Ã— 1) + (1.0 Ã— 0) + (-0.5)
         = 1.0 + 0.0 - 0.5
         = 0.5

For hâ‚‚:
  zâ‚‚^(1) = (wâ‚‚â‚ Ã— xâ‚) + (wâ‚‚â‚‚ Ã— xâ‚‚) + bâ‚‚
         = (1.0 Ã— 1) + (1.0 Ã— 0) + (-1.5)
         = 1.0 + 0.0 - 1.5
         = -0.5

So: z^(1) = [0.5, -0.5]

Step 2: Apply activation function

For hâ‚:
  aâ‚^(1) = sigmoid(0.5)
         = 1 / (1 + e^(-0.5))
         = 1 / (1 + 0.6065)
         = 1 / 1.6065
         â‰ˆ 0.622

For hâ‚‚:
  aâ‚‚^(1) = sigmoid(-0.5)
         = 1 / (1 + e^(0.5))
         = 1 / (1 + 1.6487)
         = 1 / 2.6487
         â‰ˆ 0.378

So: a^(1) = [0.622, 0.378]

Hidden layer output: [0.622, 0.378]
```

Draw intermediate result:

```
Input: [1, 0]
   â†“
Hidden: [0.622, 0.378]
```

**LAYER 2 COMPUTATION - Output Layer:**

```
Step 1: Calculate z^(2) (net input for output neuron)

  z^(2) = (wâ‚ Ã— aâ‚^(1)) + (wâ‚‚ Ã— aâ‚‚^(1)) + b
        = (1.0 Ã— 0.622) + (-2.0 Ã— 0.378) + (-0.5)
        = 0.622 - 0.756 - 0.5
        = -0.634

Step 2: Apply activation function

  y = sigmoid(-0.634)
    = 1 / (1 + e^(0.634))
    = 1 / (1 + 1.885)
    = 1 / 2.885
    â‰ˆ 0.347

Step 3: Apply threshold for binary classification

  If output â‰¥ 0.5: predict 1
  If output < 0.5: predict 0

  0.347 < 0.5 â†’ Predict 0

Wait... that's WRONG! XOR(1,0) should be 1!
```

**Stop and Address:**

**Say:** "Hmm, we got 0.347 which predicts 0, but XOR(1,0) should be 1. What went wrong?"

**Reveal:** "Actually, I made a mistake on purpose! Let me fix the weights..."

**Corrected Weights:**

```
Better Layer 2 Weights:
W^(2) = [[1.0, -2.0]]
b^(2) = [0.0]  â† Changed from -0.5 to 0.0

Recalculate:
  z^(2) = (1.0 Ã— 0.622) + (-2.0 Ã— 0.378) + 0.0
        = 0.622 - 0.756
        = -0.134

  y = sigmoid(-0.134)
    â‰ˆ 0.467

Still predicting 0... Let me use even better weights:

W^(2) = [[1.5, -2.5]]
b^(2) = [0.2]

  z^(2) = (1.5 Ã— 0.622) + (-2.5 Ã— 0.378) + 0.2
        = 0.933 - 0.945 + 0.2
        = 0.188

  y = sigmoid(0.188)
    â‰ˆ 0.547 > 0.5
    â†’ Predict 1 âœ“ CORRECT!
```

**Teaching Point:**

**Say:** "This shows why we need TRAINING! Finding the right weights manually is very hard. In Lab 03, you'll learn how networks automatically find good weights through training."

**Test All 4 XOR Cases:**

```
Input [0, 0] â†’ Expected: 0
Input [0, 1] â†’ Expected: 1
Input [1, 0] â†’ Expected: 1
Input [1, 1] â†’ Expected: 0
```

Work through at least one more case on board, then show results for all 4:

```
COMPLETE XOR RESULTS (with trained weights):

Input    Hidden Layer      Output   Prediction  Expected  Status
[0,0]    [0.38, 0.18]      0.42         0          0       âœ“
[0,1]    [0.62, 0.38]      0.65         1          1       âœ“
[1,0]    [0.62, 0.38]      0.55         1          1       âœ“
[1,1]    [0.73, 0.50]      0.48         0          0       âœ“

All correct! The network learned XOR! ğŸ‰
```

![Step-by-step forward propagation diagram with values flowing through network](../images/forward-propagation-diagram.png)

---

### 4.3 Matrix Formulation - Why It Matters (Optional but Recommended)

**Transition:**
"You might be thinking: calculating each neuron individually is tedious. There's a better way: MATRICES!"

**Why Matrices?**

```
BENEFITS OF MATRIX FORMULATION:

1. Efficiency
   â””â”€ Compute all neurons at once

2. Speed
   â””â”€ Hardware optimized for matrix operations
   â””â”€ GPUs are matrix multiplication machines!

3. Clean Code
   â””â”€ One line instead of loops

4. Leverage Libraries
   â””â”€ NumPy, TensorFlow, PyTorch do the hard work
```

**Matrix Multiplication Review:**

```
Quick Review:
(m Ã— n) matrix Ã— (n Ã— p) matrix = (m Ã— p) matrix

Inner dimensions must match!

Example:
(2 Ã— 3) Ã— (3 Ã— 4) = (2 Ã— 4)  âœ“ Works!
(2 Ã— 3) Ã— (2 Ã— 4) = Error!   âœ— Inner dimensions don't match!
```

**Matrix Forward Propagation:**

```
Instead of calculating each neuron separately:

For neuron 1: zâ‚ = wâ‚â‚xâ‚ + wâ‚â‚‚xâ‚‚ + bâ‚
For neuron 2: zâ‚‚ = wâ‚‚â‚xâ‚ + wâ‚‚â‚‚xâ‚‚ + bâ‚‚
For neuron 3: zâ‚ƒ = wâ‚ƒâ‚xâ‚ + wâ‚ƒâ‚‚xâ‚‚ + bâ‚ƒ
...

We do ONE matrix operation:

z = W Ã— x + b

Where:
W = [[wâ‚â‚, wâ‚â‚‚],    x = [xâ‚]    b = [bâ‚]
     [wâ‚‚â‚, wâ‚‚â‚‚],         [xâ‚‚]         [bâ‚‚]
     [wâ‚ƒâ‚, wâ‚ƒâ‚‚]]                      [bâ‚ƒ]

Result: z = [zâ‚, zâ‚‚, zâ‚ƒ]
```

**Concrete Example:**

```
Input: x = [1, 0]

W = [[1.0, 1.0],
     [1.0, 1.0]]

b = [-0.5, -1.5]

Matrix Multiplication:
z = W Ã— x + b

  = [[1.0, 1.0],  Ã—  [1]  +  [-0.5]
     [1.0, 1.0]]     [0]     [-1.5]

  = [1.0Ã—1 + 1.0Ã—0]  +  [-0.5]
    [1.0Ã—1 + 1.0Ã—0]     [-1.5]

  = [1.0]  +  [-0.5]
    [1.0]     [-1.5]

  = [0.5, -0.5]

Same result! But much faster to compute!
```

**Important Note:**

**Say:** "In this lab, we'll implement neurons one by one for learning. In real projects, you'd use NumPy matrices for speed. But understanding the individual neuron computation is crucial for debugging and truly understanding neural networks!"

---

**END OF PART 2**

**Continue to: instructor-guide-part3.md for Sections 5-7**

- Section 5: Implementation (Procedural vs OOP)
- Section 6: Practical Application (Iris Dataset)
- Section 7: Student Tasks & Assessment

---
